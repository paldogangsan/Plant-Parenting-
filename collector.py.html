<html>
<head>
<title>collector.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #8c8c8c; font-style: italic;}
.s1 { color: #080808;}
.s2 { color: #0033b3;}
.s3 { color: #067d17;}
.s4 { color: #1750eb;}
.s5 { color: #0037a6;}
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
collector.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot; 
The main purpose of this module is to expose LinkCollector.collect_links(). 
&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">cgi</span>
<span class="s2">import </span><span class="s1">functools</span>
<span class="s2">import </span><span class="s1">itertools</span>
<span class="s2">import </span><span class="s1">logging</span>
<span class="s2">import </span><span class="s1">mimetypes</span>
<span class="s2">import </span><span class="s1">os</span>
<span class="s2">import </span><span class="s1">re</span>
<span class="s2">from </span><span class="s1">collections </span><span class="s2">import </span><span class="s1">OrderedDict</span>

<span class="s2">from </span><span class="s1">pip._vendor </span><span class="s2">import </span><span class="s1">html5lib, requests</span>
<span class="s2">from </span><span class="s1">pip._vendor.distlib.compat </span><span class="s2">import </span><span class="s1">unescape</span>
<span class="s2">from </span><span class="s1">pip._vendor.requests.exceptions </span><span class="s2">import </span><span class="s1">RetryError, SSLError</span>
<span class="s2">from </span><span class="s1">pip._vendor.six.moves.urllib </span><span class="s2">import </span><span class="s1">parse </span><span class="s2">as </span><span class="s1">urllib_parse</span>
<span class="s2">from </span><span class="s1">pip._vendor.six.moves.urllib </span><span class="s2">import </span><span class="s1">request </span><span class="s2">as </span><span class="s1">urllib_request</span>

<span class="s2">from </span><span class="s1">pip._internal.exceptions </span><span class="s2">import </span><span class="s1">NetworkConnectionError</span>
<span class="s2">from </span><span class="s1">pip._internal.models.link </span><span class="s2">import </span><span class="s1">Link</span>
<span class="s2">from </span><span class="s1">pip._internal.models.search_scope </span><span class="s2">import </span><span class="s1">SearchScope</span>
<span class="s2">from </span><span class="s1">pip._internal.network.utils </span><span class="s2">import </span><span class="s1">raise_for_status</span>
<span class="s2">from </span><span class="s1">pip._internal.utils.compat </span><span class="s2">import </span><span class="s1">lru_cache</span>
<span class="s2">from </span><span class="s1">pip._internal.utils.filetypes </span><span class="s2">import </span><span class="s1">is_archive_file</span>
<span class="s2">from </span><span class="s1">pip._internal.utils.misc </span><span class="s2">import </span><span class="s1">pairwise, redact_auth_from_url</span>
<span class="s2">from </span><span class="s1">pip._internal.utils.typing </span><span class="s2">import </span><span class="s1">MYPY_CHECK_RUNNING</span>
<span class="s2">from </span><span class="s1">pip._internal.utils.urls </span><span class="s2">import </span><span class="s1">path_to_url, url_to_path</span>
<span class="s2">from </span><span class="s1">pip._internal.vcs </span><span class="s2">import </span><span class="s1">is_url, vcs</span>

<span class="s2">if </span><span class="s1">MYPY_CHECK_RUNNING:</span>
    <span class="s2">import </span><span class="s1">xml.etree.ElementTree</span>
    <span class="s2">from </span><span class="s1">optparse </span><span class="s2">import </span><span class="s1">Values</span>
    <span class="s2">from </span><span class="s1">typing </span><span class="s2">import </span><span class="s1">(</span>
        <span class="s1">Callable,</span>
        <span class="s1">Iterable,</span>
        <span class="s1">List,</span>
        <span class="s1">MutableMapping,</span>
        <span class="s1">Optional,</span>
        <span class="s1">Sequence,</span>
        <span class="s1">Tuple,</span>
        <span class="s1">Union,</span>
    <span class="s1">)</span>

    <span class="s2">from </span><span class="s1">pip._vendor.requests </span><span class="s2">import </span><span class="s1">Response</span>

    <span class="s2">from </span><span class="s1">pip._internal.network.session </span><span class="s2">import </span><span class="s1">PipSession</span>

    <span class="s1">HTMLElement = xml.etree.ElementTree.Element</span>
    <span class="s1">ResponseHeaders = MutableMapping[str, str]</span>


<span class="s1">logger = logging.getLogger(__name__)</span>


<span class="s2">def </span><span class="s1">_match_vcs_scheme(url):</span>
    <span class="s0"># type: (str) -&gt; Optional[str]</span>
    <span class="s0">&quot;&quot;&quot;Look for VCS schemes in the URL. 
 
    Returns the matched VCS scheme, or None if there's no match. 
    &quot;&quot;&quot;</span>
    <span class="s2">for </span><span class="s1">scheme </span><span class="s2">in </span><span class="s1">vcs.schemes:</span>
        <span class="s2">if </span><span class="s1">url.lower().startswith(scheme) </span><span class="s2">and </span><span class="s1">url[len(scheme)] </span><span class="s2">in </span><span class="s3">'+:'</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">scheme</span>
    <span class="s2">return None</span>


<span class="s2">class </span><span class="s1">_NotHTML(Exception):</span>
    <span class="s2">def </span><span class="s1">__init__(self, content_type, request_desc):</span>
        <span class="s0"># type: (str, str) -&gt; None</span>
        <span class="s1">super(_NotHTML, self).__init__(content_type, request_desc)</span>
        <span class="s1">self.content_type = content_type</span>
        <span class="s1">self.request_desc = request_desc</span>


<span class="s2">def </span><span class="s1">_ensure_html_header(response):</span>
    <span class="s0"># type: (Response) -&gt; None</span>
    <span class="s0">&quot;&quot;&quot;Check the Content-Type header to ensure the response contains HTML. 
 
    Raises `_NotHTML` if the content type is not text/html. 
    &quot;&quot;&quot;</span>
    <span class="s1">content_type = response.headers.get(</span><span class="s3">&quot;Content-Type&quot;</span><span class="s1">, </span><span class="s3">&quot;&quot;</span><span class="s1">)</span>
    <span class="s2">if not </span><span class="s1">content_type.lower().startswith(</span><span class="s3">&quot;text/html&quot;</span><span class="s1">):</span>
        <span class="s2">raise </span><span class="s1">_NotHTML(content_type, response.request.method)</span>


<span class="s2">class </span><span class="s1">_NotHTTP(Exception):</span>
    <span class="s2">pass</span>


<span class="s2">def </span><span class="s1">_ensure_html_response(url, session):</span>
    <span class="s0"># type: (str, PipSession) -&gt; None</span>
    <span class="s0">&quot;&quot;&quot;Send a HEAD request to the URL, and ensure the response contains HTML. 
 
    Raises `_NotHTTP` if the URL is not available for a HEAD request, or 
    `_NotHTML` if the content type is not text/html. 
    &quot;&quot;&quot;</span>
    <span class="s1">scheme, netloc, path, query, fragment = urllib_parse.urlsplit(url)</span>
    <span class="s2">if </span><span class="s1">scheme </span><span class="s2">not in </span><span class="s1">{</span><span class="s3">'http'</span><span class="s1">, </span><span class="s3">'https'</span><span class="s1">}:</span>
        <span class="s2">raise </span><span class="s1">_NotHTTP()</span>

    <span class="s1">resp = session.head(url, allow_redirects=</span><span class="s2">True</span><span class="s1">)</span>
    <span class="s1">raise_for_status(resp)</span>

    <span class="s1">_ensure_html_header(resp)</span>


<span class="s2">def </span><span class="s1">_get_html_response(url, session):</span>
    <span class="s0"># type: (str, PipSession) -&gt; Response</span>
    <span class="s0">&quot;&quot;&quot;Access an HTML page with GET, and return the response. 
 
    This consists of three parts: 
 
    1. If the URL looks suspiciously like an archive, send a HEAD first to 
       check the Content-Type is HTML, to avoid downloading a large file. 
       Raise `_NotHTTP` if the content type cannot be determined, or 
       `_NotHTML` if it is not HTML. 
    2. Actually perform the request. Raise HTTP exceptions on network failures. 
    3. Check the Content-Type header to make sure we got HTML, and raise 
       `_NotHTML` otherwise. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">is_archive_file(Link(url).filename):</span>
        <span class="s1">_ensure_html_response(url, session=session)</span>

    <span class="s1">logger.debug(</span><span class="s3">'Getting page %s'</span><span class="s1">, redact_auth_from_url(url))</span>

    <span class="s1">resp = session.get(</span>
        <span class="s1">url,</span>
        <span class="s1">headers={</span>
            <span class="s3">&quot;Accept&quot;</span><span class="s1">: </span><span class="s3">&quot;text/html&quot;</span><span class="s1">,</span>
            <span class="s0"># We don't want to blindly returned cached data for</span>
            <span class="s0"># /simple/, because authors generally expecting that</span>
            <span class="s0"># twine upload &amp;&amp; pip install will function, but if</span>
            <span class="s0"># they've done a pip install in the last ~10 minutes</span>
            <span class="s0"># it won't. Thus by setting this to zero we will not</span>
            <span class="s0"># blindly use any cached data, however the benefit of</span>
            <span class="s0"># using max-age=0 instead of no-cache, is that we will</span>
            <span class="s0"># still support conditional requests, so we will still</span>
            <span class="s0"># minimize traffic sent in cases where the page hasn't</span>
            <span class="s0"># changed at all, we will just always incur the round</span>
            <span class="s0"># trip for the conditional GET now instead of only</span>
            <span class="s0"># once per 10 minutes.</span>
            <span class="s0"># For more information, please see pypa/pip#5670.</span>
            <span class="s3">&quot;Cache-Control&quot;</span><span class="s1">: </span><span class="s3">&quot;max-age=0&quot;</span><span class="s1">,</span>
        <span class="s1">},</span>
    <span class="s1">)</span>
    <span class="s1">raise_for_status(resp)</span>

    <span class="s0"># The check for archives above only works if the url ends with</span>
    <span class="s0"># something that looks like an archive. However that is not a</span>
    <span class="s0"># requirement of an url. Unless we issue a HEAD request on every</span>
    <span class="s0"># url we cannot know ahead of time for sure if something is HTML</span>
    <span class="s0"># or not. However we can check after we've downloaded it.</span>
    <span class="s1">_ensure_html_header(resp)</span>

    <span class="s2">return </span><span class="s1">resp</span>


<span class="s2">def </span><span class="s1">_get_encoding_from_headers(headers):</span>
    <span class="s0"># type: (ResponseHeaders) -&gt; Optional[str]</span>
    <span class="s0">&quot;&quot;&quot;Determine if we have any encoding information in our headers. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">headers </span><span class="s2">and </span><span class="s3">&quot;Content-Type&quot; </span><span class="s2">in </span><span class="s1">headers:</span>
        <span class="s1">content_type, params = cgi.parse_header(headers[</span><span class="s3">&quot;Content-Type&quot;</span><span class="s1">])</span>
        <span class="s2">if </span><span class="s3">&quot;charset&quot; </span><span class="s2">in </span><span class="s1">params:</span>
            <span class="s2">return </span><span class="s1">params[</span><span class="s3">'charset'</span><span class="s1">]</span>
    <span class="s2">return None</span>


<span class="s2">def </span><span class="s1">_determine_base_url(document, page_url):</span>
    <span class="s0"># type: (HTMLElement, str) -&gt; str</span>
    <span class="s0">&quot;&quot;&quot;Determine the HTML document's base URL. 
 
    This looks for a ``&lt;base&gt;`` tag in the HTML document. If present, its href 
    attribute denotes the base URL of anchor tags in the document. If there is 
    no such tag (or if it does not have a valid href attribute), the HTML 
    file's URL is used as the base URL. 
 
    :param document: An HTML document representation. The current 
        implementation expects the result of ``html5lib.parse()``. 
    :param page_url: The URL of the HTML document. 
    &quot;&quot;&quot;</span>
    <span class="s2">for </span><span class="s1">base </span><span class="s2">in </span><span class="s1">document.findall(</span><span class="s3">&quot;.//base&quot;</span><span class="s1">):</span>
        <span class="s1">href = base.get(</span><span class="s3">&quot;href&quot;</span><span class="s1">)</span>
        <span class="s2">if </span><span class="s1">href </span><span class="s2">is not None</span><span class="s1">:</span>
            <span class="s2">return </span><span class="s1">href</span>
    <span class="s2">return </span><span class="s1">page_url</span>


<span class="s2">def </span><span class="s1">_clean_url_path_part(part):</span>
    <span class="s0"># type: (str) -&gt; str</span>
    <span class="s0">&quot;&quot;&quot; 
    Clean a &quot;part&quot; of a URL path (i.e. after splitting on &quot;@&quot; characters). 
    &quot;&quot;&quot;</span>
    <span class="s0"># We unquote prior to quoting to make sure nothing is double quoted.</span>
    <span class="s2">return </span><span class="s1">urllib_parse.quote(urllib_parse.unquote(part))</span>


<span class="s2">def </span><span class="s1">_clean_file_url_path(part):</span>
    <span class="s0"># type: (str) -&gt; str</span>
    <span class="s0">&quot;&quot;&quot; 
    Clean the first part of a URL path that corresponds to a local 
    filesystem path (i.e. the first part after splitting on &quot;@&quot; characters). 
    &quot;&quot;&quot;</span>
    <span class="s0"># We unquote prior to quoting to make sure nothing is double quoted.</span>
    <span class="s0"># Also, on Windows the path part might contain a drive letter which</span>
    <span class="s0"># should not be quoted. On Linux where drive letters do not</span>
    <span class="s0"># exist, the colon should be quoted. We rely on urllib.request</span>
    <span class="s0"># to do the right thing here.</span>
    <span class="s2">return </span><span class="s1">urllib_request.pathname2url(urllib_request.url2pathname(part))</span>


<span class="s0"># percent-encoded:                   /</span>
<span class="s1">_reserved_chars_re = re.compile(</span><span class="s3">'(@|%2F)'</span><span class="s1">, re.IGNORECASE)</span>


<span class="s2">def </span><span class="s1">_clean_url_path(path, is_local_path):</span>
    <span class="s0"># type: (str, bool) -&gt; str</span>
    <span class="s0">&quot;&quot;&quot; 
    Clean the path portion of a URL. 
    &quot;&quot;&quot;</span>
    <span class="s2">if </span><span class="s1">is_local_path:</span>
        <span class="s1">clean_func = _clean_file_url_path</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s1">clean_func = _clean_url_path_part</span>

    <span class="s0"># Split on the reserved characters prior to cleaning so that</span>
    <span class="s0"># revision strings in VCS URLs are properly preserved.</span>
    <span class="s1">parts = _reserved_chars_re.split(path)</span>

    <span class="s1">cleaned_parts = []</span>
    <span class="s2">for </span><span class="s1">to_clean, reserved </span><span class="s2">in </span><span class="s1">pairwise(itertools.chain(parts, [</span><span class="s3">''</span><span class="s1">])):</span>
        <span class="s1">cleaned_parts.append(clean_func(to_clean))</span>
        <span class="s0"># Normalize %xx escapes (e.g. %2f -&gt; %2F)</span>
        <span class="s1">cleaned_parts.append(reserved.upper())</span>

    <span class="s2">return </span><span class="s3">''</span><span class="s1">.join(cleaned_parts)</span>


<span class="s2">def </span><span class="s1">_clean_link(url):</span>
    <span class="s0"># type: (str) -&gt; str</span>
    <span class="s0">&quot;&quot;&quot; 
    Make sure a link is fully quoted. 
    For example, if ' ' occurs in the URL, it will be replaced with &quot;%20&quot;, 
    and without double-quoting other characters. 
    &quot;&quot;&quot;</span>
    <span class="s0"># Split the URL into parts according to the general structure</span>
    <span class="s0"># `scheme://netloc/path;parameters?query#fragment`.</span>
    <span class="s1">result = urllib_parse.urlparse(url)</span>
    <span class="s0"># If the netloc is empty, then the URL refers to a local filesystem path.</span>
    <span class="s1">is_local_path = </span><span class="s2">not </span><span class="s1">result.netloc</span>
    <span class="s1">path = _clean_url_path(result.path, is_local_path=is_local_path)</span>
    <span class="s2">return </span><span class="s1">urllib_parse.urlunparse(result._replace(path=path))</span>


<span class="s2">def </span><span class="s1">_create_link_from_element(</span>
    <span class="s1">anchor,    </span><span class="s0"># type: HTMLElement</span>
    <span class="s1">page_url,  </span><span class="s0"># type: str</span>
    <span class="s1">base_url,  </span><span class="s0"># type: str</span>
<span class="s1">):</span>
    <span class="s0"># type: (...) -&gt; Optional[Link]</span>
    <span class="s0">&quot;&quot;&quot; 
    Convert an anchor element in a simple repository page to a Link. 
    &quot;&quot;&quot;</span>
    <span class="s1">href = anchor.get(</span><span class="s3">&quot;href&quot;</span><span class="s1">)</span>
    <span class="s2">if not </span><span class="s1">href:</span>
        <span class="s2">return None</span>

    <span class="s1">url = _clean_link(urllib_parse.urljoin(base_url, href))</span>
    <span class="s1">pyrequire = anchor.get(</span><span class="s3">'data-requires-python'</span><span class="s1">)</span>
    <span class="s1">pyrequire = unescape(pyrequire) </span><span class="s2">if </span><span class="s1">pyrequire </span><span class="s2">else None</span>

    <span class="s1">yanked_reason = anchor.get(</span><span class="s3">'data-yanked'</span><span class="s1">)</span>
    <span class="s2">if </span><span class="s1">yanked_reason:</span>
        <span class="s0"># This is a unicode string in Python 2 (and 3).</span>
        <span class="s1">yanked_reason = unescape(yanked_reason)</span>

    <span class="s1">link = Link(</span>
        <span class="s1">url,</span>
        <span class="s1">comes_from=page_url,</span>
        <span class="s1">requires_python=pyrequire,</span>
        <span class="s1">yanked_reason=yanked_reason,</span>
    <span class="s1">)</span>

    <span class="s2">return </span><span class="s1">link</span>


<span class="s2">class </span><span class="s1">CacheablePageContent(object):</span>
    <span class="s2">def </span><span class="s1">__init__(self, page):</span>
        <span class="s0"># type: (HTMLPage) -&gt; None</span>
        <span class="s2">assert </span><span class="s1">page.cache_link_parsing</span>
        <span class="s1">self.page = page</span>

    <span class="s2">def </span><span class="s1">__eq__(self, other):</span>
        <span class="s0"># type: (object) -&gt; bool</span>
        <span class="s2">return </span><span class="s1">(isinstance(other, type(self)) </span><span class="s2">and</span>
                <span class="s1">self.page.url == other.page.url)</span>

    <span class="s2">def </span><span class="s1">__hash__(self):</span>
        <span class="s0"># type: () -&gt; int</span>
        <span class="s2">return </span><span class="s1">hash(self.page.url)</span>


<span class="s2">def </span><span class="s1">with_cached_html_pages(</span>
    <span class="s1">fn,    </span><span class="s0"># type: Callable[[HTMLPage], Iterable[Link]]</span>
<span class="s1">):</span>
    <span class="s0"># type: (...) -&gt; Callable[[HTMLPage], List[Link]]</span>
    <span class="s0">&quot;&quot;&quot; 
    Given a function that parses an Iterable[Link] from an HTMLPage, cache the 
    function's result (keyed by CacheablePageContent), unless the HTMLPage 
    `page` has `page.cache_link_parsing == False`. 
    &quot;&quot;&quot;</span>

    <span class="s1">@lru_cache(maxsize=</span><span class="s2">None</span><span class="s1">)</span>
    <span class="s2">def </span><span class="s1">wrapper(cacheable_page):</span>
        <span class="s0"># type: (CacheablePageContent) -&gt; List[Link]</span>
        <span class="s2">return </span><span class="s1">list(fn(cacheable_page.page))</span>

    <span class="s1">@functools.wraps(fn)</span>
    <span class="s2">def </span><span class="s1">wrapper_wrapper(page):</span>
        <span class="s0"># type: (HTMLPage) -&gt; List[Link]</span>
        <span class="s2">if </span><span class="s1">page.cache_link_parsing:</span>
            <span class="s2">return </span><span class="s1">wrapper(CacheablePageContent(page))</span>
        <span class="s2">return </span><span class="s1">list(fn(page))</span>

    <span class="s2">return </span><span class="s1">wrapper_wrapper</span>


<span class="s1">@with_cached_html_pages</span>
<span class="s2">def </span><span class="s1">parse_links(page):</span>
    <span class="s0"># type: (HTMLPage) -&gt; Iterable[Link]</span>
    <span class="s0">&quot;&quot;&quot; 
    Parse an HTML document, and yield its anchor elements as Link objects. 
    &quot;&quot;&quot;</span>
    <span class="s1">document = html5lib.parse(</span>
        <span class="s1">page.content,</span>
        <span class="s1">transport_encoding=page.encoding,</span>
        <span class="s1">namespaceHTMLElements=</span><span class="s2">False</span><span class="s1">,</span>
    <span class="s1">)</span>

    <span class="s1">url = page.url</span>
    <span class="s1">base_url = _determine_base_url(document, url)</span>
    <span class="s2">for </span><span class="s1">anchor </span><span class="s2">in </span><span class="s1">document.findall(</span><span class="s3">&quot;.//a&quot;</span><span class="s1">):</span>
        <span class="s1">link = _create_link_from_element(</span>
            <span class="s1">anchor,</span>
            <span class="s1">page_url=url,</span>
            <span class="s1">base_url=base_url,</span>
        <span class="s1">)</span>
        <span class="s2">if </span><span class="s1">link </span><span class="s2">is None</span><span class="s1">:</span>
            <span class="s2">continue</span>
        <span class="s2">yield </span><span class="s1">link</span>


<span class="s2">class </span><span class="s1">HTMLPage(object):</span>
    <span class="s0">&quot;&quot;&quot;Represents one page, along with its URL&quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self,</span>
        <span class="s1">content,                  </span><span class="s0"># type: bytes</span>
        <span class="s1">encoding,                 </span><span class="s0"># type: Optional[str]</span>
        <span class="s1">url,                      </span><span class="s0"># type: str</span>
        <span class="s1">cache_link_parsing=</span><span class="s2">True</span><span class="s1">,  </span><span class="s0"># type: bool</span>
    <span class="s1">):</span>
        <span class="s0"># type: (...) -&gt; None</span>
        <span class="s0">&quot;&quot;&quot; 
        :param encoding: the encoding to decode the given content. 
        :param url: the URL from which the HTML was downloaded. 
        :param cache_link_parsing: whether links parsed from this page's url 
                                   should be cached. PyPI index urls should 
                                   have this set to False, for example. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.content = content</span>
        <span class="s1">self.encoding = encoding</span>
        <span class="s1">self.url = url</span>
        <span class="s1">self.cache_link_parsing = cache_link_parsing</span>

    <span class="s2">def </span><span class="s1">__str__(self):</span>
        <span class="s0"># type: () -&gt; str</span>
        <span class="s2">return </span><span class="s1">redact_auth_from_url(self.url)</span>


<span class="s2">def </span><span class="s1">_handle_get_page_fail(</span>
    <span class="s1">link,  </span><span class="s0"># type: Link</span>
    <span class="s1">reason,  </span><span class="s0"># type: Union[str, Exception]</span>
    <span class="s1">meth=</span><span class="s2">None  </span><span class="s0"># type: Optional[Callable[..., None]]</span>
<span class="s1">):</span>
    <span class="s0"># type: (...) -&gt; None</span>
    <span class="s2">if </span><span class="s1">meth </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">meth = logger.debug</span>
    <span class="s1">meth(</span><span class="s3">&quot;Could not fetch URL %s: %s - skipping&quot;</span><span class="s1">, link, reason)</span>


<span class="s2">def </span><span class="s1">_make_html_page(response, cache_link_parsing=</span><span class="s2">True</span><span class="s1">):</span>
    <span class="s0"># type: (Response, bool) -&gt; HTMLPage</span>
    <span class="s1">encoding = _get_encoding_from_headers(response.headers)</span>
    <span class="s2">return </span><span class="s1">HTMLPage(</span>
        <span class="s1">response.content,</span>
        <span class="s1">encoding=encoding,</span>
        <span class="s1">url=response.url,</span>
        <span class="s1">cache_link_parsing=cache_link_parsing)</span>


<span class="s2">def </span><span class="s1">_get_html_page(link, session=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0"># type: (Link, Optional[PipSession]) -&gt; Optional[HTMLPage]</span>
    <span class="s2">if </span><span class="s1">session </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s2">raise </span><span class="s1">TypeError(</span>
            <span class="s3">&quot;_get_html_page() missing 1 required keyword argument: 'session'&quot;</span>
        <span class="s1">)</span>

    <span class="s1">url = link.url.split(</span><span class="s3">'#'</span><span class="s1">, </span><span class="s4">1</span><span class="s1">)[</span><span class="s4">0</span><span class="s1">]</span>

    <span class="s0"># Check for VCS schemes that do not support lookup as web pages.</span>
    <span class="s1">vcs_scheme = _match_vcs_scheme(url)</span>
    <span class="s2">if </span><span class="s1">vcs_scheme:</span>
        <span class="s1">logger.warning(</span><span class="s3">'Cannot look at %s URL %s because it does not support '</span>
                       <span class="s3">'lookup as web pages.'</span><span class="s1">, vcs_scheme, link)</span>
        <span class="s2">return None</span>

    <span class="s0"># Tack index.html onto file:// URLs that point to directories</span>
    <span class="s1">scheme, _, path, _, _, _ = urllib_parse.urlparse(url)</span>
    <span class="s2">if </span><span class="s1">(scheme == </span><span class="s3">'file' </span><span class="s2">and </span><span class="s1">os.path.isdir(urllib_request.url2pathname(path))):</span>
        <span class="s0"># add trailing slash if not present so urljoin doesn't trim</span>
        <span class="s0"># final segment</span>
        <span class="s2">if not </span><span class="s1">url.endswith(</span><span class="s3">'/'</span><span class="s1">):</span>
            <span class="s1">url += </span><span class="s3">'/'</span>
        <span class="s1">url = urllib_parse.urljoin(url, </span><span class="s3">'index.html'</span><span class="s1">)</span>
        <span class="s1">logger.debug(</span><span class="s3">' file: URL is directory, getting %s'</span><span class="s1">, url)</span>

    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">resp = _get_html_response(url, session=session)</span>
    <span class="s2">except </span><span class="s1">_NotHTTP:</span>
        <span class="s1">logger.warning(</span>
            <span class="s3">'Skipping page %s because it looks like an archive, and cannot '</span>
            <span class="s3">'be checked by a HTTP HEAD request.'</span><span class="s1">, link,</span>
        <span class="s1">)</span>
    <span class="s2">except </span><span class="s1">_NotHTML </span><span class="s2">as </span><span class="s1">exc:</span>
        <span class="s1">logger.warning(</span>
            <span class="s3">'Skipping page %s because the %s request got Content-Type: %s.'</span>
            <span class="s3">'The only supported Content-Type is text/html'</span><span class="s1">,</span>
            <span class="s1">link, exc.request_desc, exc.content_type,</span>
        <span class="s1">)</span>
    <span class="s2">except </span><span class="s1">NetworkConnectionError </span><span class="s2">as </span><span class="s1">exc:</span>
        <span class="s1">_handle_get_page_fail(link, exc)</span>
    <span class="s2">except </span><span class="s1">RetryError </span><span class="s2">as </span><span class="s1">exc:</span>
        <span class="s1">_handle_get_page_fail(link, exc)</span>
    <span class="s2">except </span><span class="s1">SSLError </span><span class="s2">as </span><span class="s1">exc:</span>
        <span class="s1">reason = </span><span class="s3">&quot;There was a problem confirming the ssl certificate: &quot;</span>
        <span class="s1">reason += str(exc)</span>
        <span class="s1">_handle_get_page_fail(link, reason, meth=logger.info)</span>
    <span class="s2">except </span><span class="s1">requests.ConnectionError </span><span class="s2">as </span><span class="s1">exc:</span>
        <span class="s1">_handle_get_page_fail(link, </span><span class="s3">&quot;connection error: {}&quot;</span><span class="s1">.format(exc))</span>
    <span class="s2">except </span><span class="s1">requests.Timeout:</span>
        <span class="s1">_handle_get_page_fail(link, </span><span class="s3">&quot;timed out&quot;</span><span class="s1">)</span>
    <span class="s2">else</span><span class="s1">:</span>
        <span class="s2">return </span><span class="s1">_make_html_page(resp,</span>
                               <span class="s1">cache_link_parsing=link.cache_link_parsing)</span>
    <span class="s2">return None</span>


<span class="s2">def </span><span class="s1">_remove_duplicate_links(links):</span>
    <span class="s0"># type: (Iterable[Link]) -&gt; List[Link]</span>
    <span class="s0">&quot;&quot;&quot; 
    Return a list of links, with duplicates removed and ordering preserved. 
    &quot;&quot;&quot;</span>
    <span class="s0"># We preserve the ordering when removing duplicates because we can.</span>
    <span class="s2">return </span><span class="s1">list(OrderedDict.fromkeys(links))</span>


<span class="s2">def </span><span class="s1">group_locations(locations, expand_dir=</span><span class="s2">False</span><span class="s1">):</span>
    <span class="s0"># type: (Sequence[str], bool) -&gt; Tuple[List[str], List[str]]</span>
    <span class="s0">&quot;&quot;&quot; 
    Divide a list of locations into two groups: &quot;files&quot; (archives) and &quot;urls.&quot; 
 
    :return: A pair of lists (files, urls). 
    &quot;&quot;&quot;</span>
    <span class="s1">files = []</span>
    <span class="s1">urls = []</span>

    <span class="s0"># puts the url for the given file path into the appropriate list</span>
    <span class="s2">def </span><span class="s1">sort_path(path):</span>
        <span class="s0"># type: (str) -&gt; None</span>
        <span class="s1">url = path_to_url(path)</span>
        <span class="s2">if </span><span class="s1">mimetypes.guess_type(url, strict=</span><span class="s2">False</span><span class="s1">)[</span><span class="s4">0</span><span class="s1">] == </span><span class="s3">'text/html'</span><span class="s1">:</span>
            <span class="s1">urls.append(url)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">files.append(url)</span>

    <span class="s2">for </span><span class="s1">url </span><span class="s2">in </span><span class="s1">locations:</span>

        <span class="s1">is_local_path = os.path.exists(url)</span>
        <span class="s1">is_file_url = url.startswith(</span><span class="s3">'file:'</span><span class="s1">)</span>

        <span class="s2">if </span><span class="s1">is_local_path </span><span class="s2">or </span><span class="s1">is_file_url:</span>
            <span class="s2">if </span><span class="s1">is_local_path:</span>
                <span class="s1">path = url</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">path = url_to_path(url)</span>
            <span class="s2">if </span><span class="s1">os.path.isdir(path):</span>
                <span class="s2">if </span><span class="s1">expand_dir:</span>
                    <span class="s1">path = os.path.realpath(path)</span>
                    <span class="s2">for </span><span class="s1">item </span><span class="s2">in </span><span class="s1">os.listdir(path):</span>
                        <span class="s1">sort_path(os.path.join(path, item))</span>
                <span class="s2">elif </span><span class="s1">is_file_url:</span>
                    <span class="s1">urls.append(url)</span>
                <span class="s2">else</span><span class="s1">:</span>
                    <span class="s1">logger.warning(</span>
                        <span class="s3">&quot;Path '%s' is ignored: it is a directory.&quot;</span><span class="s1">, path,</span>
                    <span class="s1">)</span>
            <span class="s2">elif </span><span class="s1">os.path.isfile(path):</span>
                <span class="s1">sort_path(path)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">logger.warning(</span>
                    <span class="s3">&quot;Url '%s' is ignored: it is neither a file &quot;</span>
                    <span class="s3">&quot;nor a directory.&quot;</span><span class="s1">, url,</span>
                <span class="s1">)</span>
        <span class="s2">elif </span><span class="s1">is_url(url):</span>
            <span class="s0"># Only add url with clear scheme</span>
            <span class="s1">urls.append(url)</span>
        <span class="s2">else</span><span class="s1">:</span>
            <span class="s1">logger.warning(</span>
                <span class="s3">&quot;Url '%s' is ignored. It is either a non-existing &quot;</span>
                <span class="s3">&quot;path or lacks a specific scheme.&quot;</span><span class="s1">, url,</span>
            <span class="s1">)</span>

    <span class="s2">return </span><span class="s1">files, urls</span>


<span class="s2">class </span><span class="s1">CollectedLinks(object):</span>

    <span class="s0">&quot;&quot;&quot; 
    Encapsulates the return value of a call to LinkCollector.collect_links(). 
 
    The return value includes both URLs to project pages containing package 
    links, as well as individual package Link objects collected from other 
    sources. 
 
    This info is stored separately as: 
 
    (1) links from the configured file locations, 
    (2) links from the configured find_links, and 
    (3) urls to HTML project pages, as described by the PEP 503 simple 
        repository API. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self,</span>
        <span class="s1">files,         </span><span class="s0"># type: List[Link]</span>
        <span class="s1">find_links,    </span><span class="s0"># type: List[Link]</span>
        <span class="s1">project_urls,  </span><span class="s0"># type: List[Link]</span>
    <span class="s1">):</span>
        <span class="s0"># type: (...) -&gt; None</span>
        <span class="s0">&quot;&quot;&quot; 
        :param files: Links from file locations. 
        :param find_links: Links from find_links. 
        :param project_urls: URLs to HTML project pages, as described by 
            the PEP 503 simple repository API. 
        &quot;&quot;&quot;</span>
        <span class="s1">self.files = files</span>
        <span class="s1">self.find_links = find_links</span>
        <span class="s1">self.project_urls = project_urls</span>


<span class="s2">class </span><span class="s1">LinkCollector(object):</span>

    <span class="s0">&quot;&quot;&quot; 
    Responsible for collecting Link objects from all configured locations, 
    making network requests as needed. 
 
    The class's main method is its collect_links() method. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__(</span>
        <span class="s1">self,</span>
        <span class="s1">session,       </span><span class="s0"># type: PipSession</span>
        <span class="s1">search_scope,  </span><span class="s0"># type: SearchScope</span>
    <span class="s1">):</span>
        <span class="s0"># type: (...) -&gt; None</span>
        <span class="s1">self.search_scope = search_scope</span>
        <span class="s1">self.session = session</span>

    <span class="s1">@classmethod</span>
    <span class="s2">def </span><span class="s1">create(cls, session, options, suppress_no_index=</span><span class="s2">False</span><span class="s1">):</span>
        <span class="s0"># type: (PipSession, Values, bool) -&gt; LinkCollector</span>
        <span class="s0">&quot;&quot;&quot; 
        :param session: The Session to use to make requests. 
        :param suppress_no_index: Whether to ignore the --no-index option 
            when constructing the SearchScope object. 
        &quot;&quot;&quot;</span>
        <span class="s1">index_urls = [options.index_url] + options.extra_index_urls</span>
        <span class="s2">if </span><span class="s1">options.no_index </span><span class="s2">and not </span><span class="s1">suppress_no_index:</span>
            <span class="s1">logger.debug(</span>
                <span class="s3">'Ignoring indexes: %s'</span><span class="s1">,</span>
                <span class="s3">','</span><span class="s1">.join(redact_auth_from_url(url) </span><span class="s2">for </span><span class="s1">url </span><span class="s2">in </span><span class="s1">index_urls),</span>
            <span class="s1">)</span>
            <span class="s1">index_urls = []</span>

        <span class="s0"># Make sure find_links is a list before passing to create().</span>
        <span class="s1">find_links = options.find_links </span><span class="s2">or </span><span class="s1">[]</span>

        <span class="s1">search_scope = SearchScope.create(</span>
            <span class="s1">find_links=find_links, index_urls=index_urls,</span>
        <span class="s1">)</span>
        <span class="s1">link_collector = LinkCollector(</span>
            <span class="s1">session=session, search_scope=search_scope,</span>
        <span class="s1">)</span>
        <span class="s2">return </span><span class="s1">link_collector</span>

    <span class="s1">@property</span>
    <span class="s2">def </span><span class="s1">find_links(self):</span>
        <span class="s0"># type: () -&gt; List[str]</span>
        <span class="s2">return </span><span class="s1">self.search_scope.find_links</span>

    <span class="s2">def </span><span class="s1">fetch_page(self, location):</span>
        <span class="s0"># type: (Link) -&gt; Optional[HTMLPage]</span>
        <span class="s0">&quot;&quot;&quot; 
        Fetch an HTML page containing package links. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">_get_html_page(location, session=self.session)</span>

    <span class="s2">def </span><span class="s1">collect_links(self, project_name):</span>
        <span class="s0"># type: (str) -&gt; CollectedLinks</span>
        <span class="s0">&quot;&quot;&quot;Find all available links for the given project name. 
 
        :return: All the Link objects (unfiltered), as a CollectedLinks object. 
        &quot;&quot;&quot;</span>
        <span class="s1">search_scope = self.search_scope</span>
        <span class="s1">index_locations = search_scope.get_index_urls_locations(project_name)</span>
        <span class="s1">index_file_loc, index_url_loc = group_locations(index_locations)</span>
        <span class="s1">fl_file_loc, fl_url_loc = group_locations(</span>
            <span class="s1">self.find_links, expand_dir=</span><span class="s2">True</span><span class="s1">,</span>
        <span class="s1">)</span>

        <span class="s1">file_links = [</span>
            <span class="s1">Link(url) </span><span class="s2">for </span><span class="s1">url </span><span class="s2">in </span><span class="s1">itertools.chain(index_file_loc, fl_file_loc)</span>
        <span class="s1">]</span>

        <span class="s0"># We trust every directly linked archive in find_links</span>
        <span class="s1">find_link_links = [Link(url, </span><span class="s3">'-f'</span><span class="s1">) </span><span class="s2">for </span><span class="s1">url </span><span class="s2">in </span><span class="s1">self.find_links]</span>

        <span class="s0"># We trust every url that the user has given us whether it was given</span>
        <span class="s0"># via --index-url or --find-links.</span>
        <span class="s0"># We want to filter out anything that does not have a secure origin.</span>
        <span class="s1">url_locations = [</span>
            <span class="s1">link </span><span class="s2">for </span><span class="s1">link </span><span class="s2">in </span><span class="s1">itertools.chain(</span>
                <span class="s0"># Mark PyPI indices as &quot;cache_link_parsing == False&quot; -- this</span>
                <span class="s0"># will avoid caching the result of parsing the page for links.</span>
                <span class="s1">(Link(url, cache_link_parsing=</span><span class="s2">False</span><span class="s1">) </span><span class="s2">for </span><span class="s1">url </span><span class="s2">in </span><span class="s1">index_url_loc),</span>
                <span class="s1">(Link(url) </span><span class="s2">for </span><span class="s1">url </span><span class="s2">in </span><span class="s1">fl_url_loc),</span>
            <span class="s1">)</span>
            <span class="s2">if </span><span class="s1">self.session.is_secure_origin(link)</span>
        <span class="s1">]</span>

        <span class="s1">url_locations = _remove_duplicate_links(url_locations)</span>
        <span class="s1">lines = [</span>
            <span class="s3">'{} location(s) to search for versions of {}:'</span><span class="s1">.format(</span>
                <span class="s1">len(url_locations), project_name,</span>
            <span class="s1">),</span>
        <span class="s1">]</span>
        <span class="s2">for </span><span class="s1">link </span><span class="s2">in </span><span class="s1">url_locations:</span>
            <span class="s1">lines.append(</span><span class="s3">'* {}'</span><span class="s1">.format(link))</span>
        <span class="s1">logger.debug(</span><span class="s3">'</span><span class="s5">\n</span><span class="s3">'</span><span class="s1">.join(lines))</span>

        <span class="s2">return </span><span class="s1">CollectedLinks(</span>
            <span class="s1">files=file_links,</span>
            <span class="s1">find_links=find_link_links,</span>
            <span class="s1">project_urls=url_locations,</span>
        <span class="s1">)</span>
</pre>
</body>
</html>